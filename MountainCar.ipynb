{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\dimna\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from gym) (1.18.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from gym) (7.0.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: torch in c:\\users\\dimna\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from torch) (1.18.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from torch) (3.10.0.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.9.1-cp37-cp37m-win_amd64.whl (852 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from torchvision) (1.18.1)\n",
      "Requirement already satisfied: torch==1.8.1 in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dimna\\anaconda3\\lib\\site-packages (from torch==1.8.1->torchvision) (3.10.0.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = nn.Sequential(\n",
    "    nn.Linear(2, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', 'state action reward next_state done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.index = 0\n",
    "        \n",
    "    \n",
    "    def push(self, x):\n",
    "        self.memory[self.index] = x\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    \n",
    "    def full(self):\n",
    "        return len(self.memory) == self.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, input_dim, hidden_layers_dim, output_dim, lr, optimizer, loss_function\n",
    "                 , gamma, eps, eps_decay, eps_min_bound, cache_size, batch_size):\n",
    "        \n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_layers_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers_dim, hidden_layers_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers_dim, output_dim)\n",
    "        )\n",
    "        self.target_net = copy.deepcopy(self.policy_net)\n",
    "        \n",
    "        self.policy_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        \n",
    "        self.optimizer = optimizer(self.policy_net.parameters(), lr=lr)\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers_dim = hidden_layers_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min_bound = eps_min_bound\n",
    "        \n",
    "        self.cache = Cache(cache_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        self.eps *= self.eps_decay\n",
    "        self.eps = max(self.eps, self.eps_min_bound)\n",
    "        \n",
    "        if random.random() < self.eps:\n",
    "            return random.randint(0, self.output_dim)\n",
    "        return self.policy_net().argmax().item()\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.stack([b.state for b in batch]).to(device)\n",
    "        next_states = torch.stack([b.next_state for b in batch]).to(device)\n",
    "        \n",
    "        actions = torch.tensor([b.action for b in batch]).to(device)\n",
    "        dones = torch.tensor([b.done] for b in batch).to(device)\n",
    "        \n",
    "        rewards = torch.tensor([b.rewards for b in batch]).to(device).float()\n",
    "        \n",
    "        target_q = torch.zeros(len(batch_size)).to(device).float()\n",
    "        with torch.no_grad():\n",
    "            target_q = target_net(next_states).max(1)[0]\n",
    "        target_q[done] = 0\n",
    "        target_q = rewards + target_q * self.gamma\n",
    "        \n",
    "        actual_q = policy_net(states).gather(1, actions)\n",
    "        \n",
    "        print(target_q, actual_q)\n",
    "        exit()\n",
    "        \n",
    "        loss = self.loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(2, 16, 3, 0.0003, optim.Adam, F.smooth_l1_loss, 0.99, 0.9, 0.999, 0.1, 5000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transition(state=1, action=2, reward=3, next_state=4, done=5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_games = 20\n",
    "for i in range(n_games):\n",
    "    state = env.reset()\n",
    "    state\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.cache.store_transition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
